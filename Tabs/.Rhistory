#   geom_point() +
#   scale_y_discrete(name = "Spending score") +
#   scale_x_continuous(name = "Income per year") +
#   facet_wrap(~Gender) +
#   theme_bw() +
#   scale_color_brewer(type = "qual", palette = 6 ))
library(fpc)
plotcluster(norm_data[-c(1,2)], norm_data$cluster)
ggplotly(ggplot(norm_data, aes(annual_income, spending_score, col = cluster)) +
geom_point() +
scale_y_discrete(name = "Spending score") +
scale_x_continuous(name = "Income per year") +
facet_wrap(~Gender) +
theme_bw() +
scale_color_brewer(type = "qual", palette = 6 ))
data2 <- data.frame(X =c(0, 1, 2, 3), Y = c(0, 1, 0, 3))
data2
fit <- lm(Y ~ X, data2)
fit
install.packages("glmnet")
library("glmnet", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
library("plyr", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
library("caret", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
install.packages("moments")
library("moments", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
install.packages("elasticnet")
library("elasticnet", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
library("knitr", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
model_ridge <- train(x=data2$X,y=data2$Y,
method="glmnet",
metric="RMSE",
maximize=FALSE,
trControl=CARET.TRAIN.CTRL,
tuneGrid=expand.grid(alpha=0, # Ridge regression
lambda=lambdas))
model_ridge <- train(x=X,y=Y,
method="glmnet",
metric="RMSE",
maximize=FALSE,
trControl=CARET.TRAIN.CTRL,
tuneGrid=expand.grid(alpha=0, # Ridge regression
lambda=lambdas))
X =c(0, 1, 2, 3)
Y = c(0, 1, 0, 3)
model_ridge <- train(x=X,y=Y,
method="glmnet",
metric="RMSE",
maximize=FALSE,
trControl=CARET.TRAIN.CTRL,
tuneGrid=expand.grid(alpha=0, # Ridge regression
lambda=lambdas))
model_ridge <- train(x='X',y='Y',
method="glmnet",
metric="RMSE",
maximize=FALSE,
trControl=CARET.TRAIN.CTRL,
tuneGrid=expand.grid(alpha=0, # Ridge regression
lambda=lambdas))
model_ridge <- train(x=X,y=Y,
method="glmnet",
metric="RMSE",
maximize=FALSE,
trControl=CARET.TRAIN.CTRL,
tuneGrid=expand.grid(alpha=0, # Ridge regression
lambda=lambdas))
glmnet(x = X, y = Y, alpha = 1)
data2 <- data.frame(X =c(0, 1, 2, 3), Y = c(0, 1, 0, 3))
glmnet(x = data2$X, y = data2$Y, alpha = 1)
data2 <- matrix(X =c(0, 1, 2, 3), Y = c(0, 1, 0, 3))
data2 <- matrix(data =c(0, 1, 2, 3, 3, 2, 1, 0))
data2 <- matrix(data =c(0, 1, 2, 3, 3, 2, 1, 0), ncol = 2)
data2
y = c(0, 1, 0, 3)
# model_ridge <- train(x=X,y=Y,
#                      method="glmnet",
#                      metric="RMSE",
#                      maximize=FALSE,
#                      trControl=CARET.TRAIN.CTRL,
#                      tuneGrid=expand.grid(alpha=0, # Ridge regression
#                                           lambda=lambdas))
glmnet(x = data2, y = y, alpha = 1)
# model_ridge <- train(x=X,y=Y,
#                      method="glmnet",
#                      metric="RMSE",
#                      maximize=FALSE,
#                      trControl=CARET.TRAIN.CTRL,
#                      tuneGrid=expand.grid(alpha=0, # Ridge regression
#                                           lambda=lambdas))
glmnet(x = data2, y = y, alpha = 0)
# model_ridge <- train(x=X,y=Y,
#                      method="glmnet",
#                      metric="RMSE",
#                      maximize=FALSE,
#                      trControl=CARET.TRAIN.CTRL,
#                      tuneGrid=expand.grid(alpha=0, # Ridge regression
#                                           lambda=lambdas))
glmnet(x = data2, y = y, alpha = 1)
# model_ridge <- train(x=X,y=Y,
#                      method="glmnet",
#                      metric="RMSE",
#                      maximize=FALSE,
#                      trControl=CARET.TRAIN.CTRL,
#                      tuneGrid=expand.grid(alpha=0, # Ridge regression
#                                           lambda=lambdas))
glmnet(x = data2, y = y, lambda = 1)
# model_ridge <- train(x=X,y=Y,
#                      method="glmnet",
#                      metric="RMSE",
#                      maximize=FALSE,
#                      trControl=CARET.TRAIN.CTRL,
#                      tuneGrid=expand.grid(alpha=0, # Ridge regression
#                                           lambda=lambdas))
summary(glmnet(x = data2, y = y, lambda = 1))
# model_ridge <- train(x=X,y=Y,
#                      method="glmnet",
#                      metric="RMSE",
#                      maximize=FALSE,
#                      trControl=CARET.TRAIN.CTRL,
#                      tuneGrid=expand.grid(alpha=0, # Ridge regression
#                                           lambda=lambdas))
fit2 <- glmnet(x = data2, y = y, lambda = 1)
fit2$beta
fit2$a0
# model_ridge <- train(x=X,y=Y,
#                      method="glmnet",
#                      metric="RMSE",
#                      maximize=FALSE,
#                      trControl=CARET.TRAIN.CTRL,
#                      tuneGrid=expand.grid(alpha=0, # Ridge regression
#                                           lambda=lambdas))
fit2 <- glmnet(x = data2, y = y, lambda = 1)
# model_ridge <- train(x=X,y=Y,
#                      method="glmnet",
#                      metric="RMSE",
#                      maximize=FALSE,
#                      trControl=CARET.TRAIN.CTRL,
#                      tuneGrid=expand.grid(alpha=0, # Ridge regression
#                                           lambda=lambdas))
fit2 <- glmnet(x = data2, y = y, lambda = 1)
fit2
# model_ridge <- train(x=X,y=Y,
#                      method="glmnet",
#                      metric="RMSE",
#                      maximize=FALSE,
#                      trControl=CARET.TRAIN.CTRL,
#                      tuneGrid=expand.grid(alpha=0, # Ridge regression
#                                           lambda=lambdas))
fit2 <- glmnet(x = data2, y = y, lambda = 1, alpha = 0)
fit2
summary(fit2)
preditc(fit2, type = 'coefficients')
predict(fit2, type = 'coefficients')
# model_ridge <- train(x=X,y=Y,
#                      method="glmnet",
#                      metric="RMSE",
#                      maximize=FALSE,
#                      trControl=CARET.TRAIN.CTRL,
#                      tuneGrid=expand.grid(alpha=0, # Ridge regression
#                                           lambda=lambdas))
fit2 <- glmnet(x = data2, y = y, lambda = 1)
predict(fit2, type = 'coefficients')
# model_ridge <- train(x=X,y=Y,
#                      method="glmnet",
#                      metric="RMSE",
#                      maximize=FALSE,
#                      trControl=CARET.TRAIN.CTRL,
#                      tuneGrid=expand.grid(alpha=0, # Ridge regression
#                                           lambda=lambdas))
fit2 <- glmnet(x = data2, y = y, lambda = 1, alpha = 0)
predict(fit2, type = 'coefficients', s = bestlam)
# model_ridge <- train(x=X,y=Y,
#                      method="glmnet",
#                      metric="RMSE",
#                      maximize=FALSE,
#                      trControl=CARET.TRAIN.CTRL,
#                      tuneGrid=expand.grid(alpha=0, # Ridge regression
#                                           lambda=lambdas))
fit2 <- glmnet(x = data2, y = y, lambda = 1, alpha = 0)
fit2
predict(fit2, type = 'coefficients', s = 0)
predict(fit2, type = 'coefficients', s = 0, exact = T)
coef(fit2)
coef(fit2, exact = T)
data2
# model_ridge <- train(x=X,y=Y,
#                      method="glmnet",
#                      metric="RMSE",
#                      maximize=FALSE,
#                      trControl=CARET.TRAIN.CTRL,
#                      tuneGrid=expand.grid(alpha=0, # Ridge regression
#                                           lambda=lambdas))
fit2 <- glmnet(x = data2, y = y, lambda = 1, alpha = 0)
coef(fit2, exact = T)
predict(fit2, type = 'coefficients', s = 0, exact = T)
predict(fit2, type = 'coefficients', s = 0)
install.packages("ridge")
library("ridge", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
dataZZ = data.frame(X1 = c(0, 1, 2, 3), X2 = c(3, 2, 1, 0), Y = c(0, 1, 0, 3))
linRidgeMod <- linearRidge(Y ~ ., data = dataZZ)
linRidgeMod
linRidgeMod <- linearRidge(Y ~ ., data = dataZZ, lambda = 1)
linRidgeMod
# model_ridge <- train(x=X,y=Y,
#                      method="glmnet",
#                      metric="RMSE",
#                      maximize=FALSE,
#                      trControl=CARET.TRAIN.CTRL,
#                      tuneGrid=expand.grid(alpha=0, # Ridge regression
#                                           lambda=lambdas))
fit2 <- glmnet(x = data2, y = y, lambda = 1, alpha = 1)
coef(fit2, exact = T)
# model_ridge <- train(x=X,y=Y,
#                      method="glmnet",
#                      metric="RMSE",
#                      maximize=FALSE,
#                      trControl=CARET.TRAIN.CTRL,
#                      tuneGrid=expand.grid(alpha=0, # Ridge regression
#                                           lambda=lambdas))
fit2 <- glmnet(x = data2, y = y, lambda = 1, alpha = 0)
coef(fit2, exact = T)
linRidgeMod <- linearRidge(Y ~ ., data = dataZZ, lambda = 1)
linRidgeMod
# train the model for each parameter combination in the grid,
#   using CV to evaluate
xgb_train_1 = train(
x = as.matrix(diamonds_n[-4]),
y = as.vector(diamonds_n$price),
trControl = xgb_trcontrol_1,
tuneGrid = xgb_grid_1,
method = "xgbLinear"
)
library("caret", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
# set up the cross-validated hyper-parameter search
xgb_grid_1 = expand.grid(
nrounds = 1000,
eta = c(0.01, 0.001, 0.0001),
lambda = 0,
alpha = 0
)
# pack the training control parameters
xgb_trcontrol_1 = trainControl(
method = "cv",
number = 5,
allowParallel = TRUE
)
# train the model for each parameter combination in the grid,
#   using CV to evaluate
xgb_train_1 = train(
x = as.matrix(diamonds_n[-4]),
y = as.vector(diamonds_n$price),
trControl = xgb_trcontrol_1,
tuneGrid = xgb_grid_1,
method = "xgbLinear"
)
diamonds <- diamonds
summary(diamonds)
diamonds_n <- diamonds[-c(2, 3, 4)]
# set up the cross-validated hyper-parameter search
xgb_grid_1 = expand.grid(
nrounds = 1000,
eta = c(0.01, 0.001, 0.0001),
lambda = 0,
alpha = 0
)
# pack the training control parameters
xgb_trcontrol_1 = trainControl(
method = "cv",
number = 5,
allowParallel = TRUE
)
# train the model for each parameter combination in the grid,
#   using CV to evaluate
xgb_train_1 = train(
x = as.matrix(diamonds_n[-4]),
y = as.vector(diamonds_n$price),
trControl = xgb_trcontrol_1,
tuneGrid = xgb_grid_1,
method = "xgbLinear"
)
# set up the cross-validated hyper-parameter search
xgb_grid_1 = expand.grid(
nrounds = 10,
eta = c(0.01, 0.001, 0.0001),
lambda = 0,
alpha = 0
)
# pack the training control parameters
xgb_trcontrol_1 = trainControl(
method = "cv",
number = 5,
allowParallel = TRUE
)
# train the model for each parameter combination in the grid,
#   using CV to evaluate
xgb_train_1 = train(
x = as.matrix(diamonds_n[-4]),
y = as.vector(diamonds_n$price),
trControl = xgb_trcontrol_1,
tuneGrid = xgb_grid_1,
method = "xgbLinear"
)
xgb_train_1$results
# scatter plot of the AUC against max_depth and eta
ggplot(xgb_train_1$results, aes(x = as.factor(eta), y = max_depth, size = ROC, color = ROC)) +
geom_point() +
theme_bw() +
scale_size_continuous(guide = "none")
# set up the cross-validated hyper-parameter search
xgb_grid_1 = expand.grid(
nrounds = 10,
eta = c(0.01, 0.001, 0.0001),
lambda = c(0, 0.1, 0.01, 0.5, 0.7),
alpha = c(0, 0.1, 0.01, 0.5, 0.7)
)
# pack the training control parameters
xgb_trcontrol_1 = trainControl(
method = "cv",
number = 5,
allowParallel = TRUE
)
# train the model for each parameter combination in the grid,
#   using CV to evaluate
xgb_train_1 = train(
x = as.matrix(diamonds_n[-4]),
y = as.vector(diamonds_n$price),
trControl = xgb_trcontrol_1,
tuneGrid = xgb_grid_1,
method = "xgbLinear"
)
xgb_train_1
xgb_train_1$results
# train the model for each parameter combination in the grid,
#   using CV to evaluate
xgb_train_1 = train(metric = "ROC",
x = as.matrix(diamonds_n[-4]),
y = as.vector(diamonds_n$price),
trControl = xgb_trcontrol_1,
tuneGrid = xgb_grid_1,
method = "xgbLinear"
)
# train the model for each parameter combination in the grid,
#   using CV to evaluate
xgb_train_1 = train(metric = "AUC",
x = as.matrix(diamonds_n[-4]),
y = as.vector(diamonds_n$price),
trControl = xgb_trcontrol_1,
tuneGrid = xgb_grid_1,
method = "xgbLinear"
)
xgb_train_1
xgb_train_1$modelType
xgb_train_1$pred
xgb_train_1$metric
xgb_train_1$levels
xgb_train_1$preProcess
xgb_train_1$call
# train the model for each parameter combination in the grid,
#   using CV to evaluate
xgb_train_1 = train(metric = "AUC",
x = as.matrix(diamonds_n[-4]),
y = as.vector(diamonds_n$price),
trControl = xgb_trcontrol_1,
# tuneGrid = xgb_grid_1,
method = "xgbLinear"
)
# pack the training control parameters
xgb_trcontrol_1 = trainControl(
method = "cv",
number = 5,
allowParallel = TRUE,
summaryFunction = twoClassSummary
)
# train the model for each parameter combination in the grid,
#   using CV to evaluate
xgb_train_1 = train(metric = "AUC",
x = as.matrix(diamonds_n[-4]),
y = as.vector(diamonds_n$price),
trControl = xgb_trcontrol_1,
# tuneGrid = xgb_grid_1,
method = "xgbLinear"
)
# train the model for each parameter combination in the grid,
#   using CV to evaluate
xgb_train_1 = train(metric = "AUC",
x = as.matrix(diamonds_n[-4]),
y = as.vector(diamonds_n$price),
trControl = xgb_trcontrol_1,
tuneGrid = xgb_grid_1,
method = "xgbLinear"
)
xgb_train_1
# pack the training control parameters
xgb_trcontrol_1 = trainControl(
method = "cv",
number = 5,
allowParallel = TRUE
)
# train the model for each parameter combination in the grid,
#   using CV to evaluate
xgb_train_1 = train(metric = "AUC",
x = as.matrix(diamonds_n[-4]),
y = as.vector(diamonds_n$price),
trControl = xgb_trcontrol_1,
tuneGrid = xgb_grid_1,
method = "xgbLinear"
)
xgb_train_1$yLimits
cor.test(x = diamonds_n)
cor(x = diamonds_n)
# set up the cross-validated hyper-parameter search
xgb_grid_1 = expand.grid(
nrounds = 10,
eta = c(0.01, 0.001, 0.0001, 0.2, 0.3, 0.4),
lambda = c(0, 0.1, 0.01, 0.5, 0.7, 0.8, 1),
alpha = c(0, 0.1, 0.01, 0.5, 0.7, 0.8, 1)
)
# pack the training control parameters
xgb_trcontrol_1 = trainControl(
method = "cv",
number = 5,
allowParallel = TRUE
)
# train the model for each parameter combination in the grid,
#   using CV to evaluate
xgb_train_1 = train(
x = as.matrix(diamonds_n[-4]),
y = as.vector(diamonds_n$price),
trControl = xgb_trcontrol_1,
tuneGrid = xgb_grid_1,
method = "xgbLinear"
)
xgb_train_1$finalModel
xgb_train_1$tuneValue
xgb_train_1$bestTune
hours(00:10)
hours(1)
library("lubridate", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
hours(1)
test_time<- ymd("2011-01-01 23:00:00")+ hours(1)
ymd("2011-01-01 23:00:00")+ hours(1)
ymd_hms("2011-01-01 23:00:00")+ hours(1)
ymd_hms("2011-01-01 23:00:00")+ hours(2)
hours = 1:24
sapply(hours, function (x) ymd_hms("2011-01-01 23:00:00") + x)
sapply(hours, function (x) ymd_hms("2011-01-01 23:00:00") + hours(x))
sapply(1:24, function (x) ymd_hms("2011-01-01 23:00:00") + hours(x))
x = 1:24
for (i in x){
print(ymd_hms("2011-01-01 23:00:00") + hours(i))
}
x = 1:46
for (i in x){
print(ymd_hms("2011-01-01 23:00:00") + hours(i))
}
sapply(1:24, function (x) ymd_hms("2011-01-01 23:00:00") + hours(x))
as.POSIXct(sapply(1:24, function (x) ymd_hms("2011-01-01 23:00:00") + hours(x)))
as.POSIXct(sapply(1:24, function (x) ymd_hms("2011-01-01 23:00:00") + hours(x)), origin = "1970-01-01")
as.POSIXct(sapply(1:50, function (x) ymd_hms("2011-01-01 23:00:00") + hours(x)), origin = "1970-01-01")
as.POSIXct(sapply(1:50, function (x) ymd_hms("2011-01-01 22:00:00") + hours(x)), origin = "1970-01-01")
as.POSIXct(sapply(1:24, function (x) ymd_hms("2011-01-01 22:00:00") + hours(x)), origin = "1970-01-01")
as.POSIXct(sapply(0:23, function (x) ymd_hms("2011-01-01 22:00:00") + hours(x)), origin = "1970-01-01")
hours(0)
# x = 1:46
as.POSIXct(sapply(0:23, function (x) ymd_hms("2011-01-01 22:00:00") + hours(x)), origin = "1970-01-01")
# x = 1:46
as.POSIXct(sapply(0:23, function (x) ymd_hms("2011-01-01 23:00:00") + hours(x)), origin = "1970-01-01")
# x = 1:46
as.POSIXct(sapply(1:23, function (x) ymd_hms("2011-01-01 23:00:00") + hours(x)), origin = "1970-01-01")
# x = 1:46
as.POSIXct(sapply(0:23, function (x) ymd_hms("2011-01-01 23:00:00") + hours(x)), origin = "1970-01-01 00:00:00")
# x = 1:46
as.POSIXct(sapply(0:23, function (x) ymd_hms("2011-01-01 22:00:00") + hours(x)), origin = "1970-01-01")
ymd_hms("2011-01-01 22:00:00") + hours(0)
ymd_hms("2011-01-01 22:00:00") + hours(1)
# x = 1:46
as.POSIXct(sapply(0:23, function (x) ymd_hms("2011-01-01 22:00:00") + hours(x)), origin = "1970-01-01", tz = "UTC")
# x = 1:46
as.POSIXct(sapply(1:24, function (x) ymd_hms("2011-01-01 22:00:00") + hours(x)), origin = "1970-01-01", tz = "UTC")
# x = 1:46
as.POSIXct(sapply(1:24, function (x) ymd_hms("2011-01-01 23:00:00") + hours(x)), origin = "1970-01-01", tz = "UTC")
# x = 1:46
as.POSIXct(sapply(1:24, function (x) ymd_hms("2011-01-01 23:00:00") + hours(x)), origin = "1970-01-01", tz = "UTC")
# x = 1:46
as.POSIXct(sapply(1:24, function (x) ymd_hms("2011-01-01 20:00:00") + hours(x)), origin = "1970-01-01", tz = "UTC")
# test_time<- ymd_hms("2011-01-01 23:00:00")+ hours(2)
ymd_hms("2011-01-01 23:00:00" )+ hours(1)
# test_time<- ymd_hms("2011-01-01 23:00:00")+ hours(2)
ymd("2011-01-01") +  days(1:10)
# test_time<- ymd_hms("2011-01-01 23:00:00")+ hours(2)
ymd("2011-01-01 23:00:00") +  days(1:10)
# test_time<- ymd_hms("2011-01-01 23:00:00")+ hours(2)
ymd_hms("2011-01-01 23:00:00") + hours(1:10)
# test_time<- ymd_hms("2011-01-01 23:00:00")+ hours(2)
ymd_hms("2011-01-01 23:00:00") + hours(0:10)
# test_time<- ymd_hms("2011-01-01 23:00:00")+ hours(2)
ymd_hms("2011-01-01 23:00:00") + hours(00:10)
# test_time<- ymd_hms("2011-01-01 23:00:00")+ hours(2)
ymd_hms("2011-01-01 23:00:00") + hours(0:10)
sapply(1:24, function (x) ymd_hms("2011-01-01 20:00:00") + hours(x))
sapply(1:24, function (x) ymd_hms("2011-01-01 20:00:00") + hours(x))
# x = 1:46
as.POSIXct(sapply(1:24, function (x) ymd_hms("2011-01-01 20:00:00") + hours(x)), origin = "1970-01-01", tz = "UTC")
shiny::runApp('Shiny/Tables')
runApp()
setwd("~/Shiny/Tabs")
runApp()
runApp('~/Shiny/Interactive_Graphs')
trees <- trees
View(trees)
View(trees)
source('~/R_tasks/draft.R')
runApp('~/Shiny/Interactive_Graphs')
