# model_ridge <- train(x=X,y=Y,
#                      method="glmnet",
#                      metric="RMSE",
#                      maximize=FALSE,
#                      trControl=CARET.TRAIN.CTRL,
#                      tuneGrid=expand.grid(alpha=0, # Ridge regression
#                                           lambda=lambdas))
glmnet(x = data2, y = y, alpha = 0)
# model_ridge <- train(x=X,y=Y,
#                      method="glmnet",
#                      metric="RMSE",
#                      maximize=FALSE,
#                      trControl=CARET.TRAIN.CTRL,
#                      tuneGrid=expand.grid(alpha=0, # Ridge regression
#                                           lambda=lambdas))
glmnet(x = data2, y = y, alpha = 1)
# model_ridge <- train(x=X,y=Y,
#                      method="glmnet",
#                      metric="RMSE",
#                      maximize=FALSE,
#                      trControl=CARET.TRAIN.CTRL,
#                      tuneGrid=expand.grid(alpha=0, # Ridge regression
#                                           lambda=lambdas))
glmnet(x = data2, y = y, lambda = 1)
# model_ridge <- train(x=X,y=Y,
#                      method="glmnet",
#                      metric="RMSE",
#                      maximize=FALSE,
#                      trControl=CARET.TRAIN.CTRL,
#                      tuneGrid=expand.grid(alpha=0, # Ridge regression
#                                           lambda=lambdas))
summary(glmnet(x = data2, y = y, lambda = 1))
# model_ridge <- train(x=X,y=Y,
#                      method="glmnet",
#                      metric="RMSE",
#                      maximize=FALSE,
#                      trControl=CARET.TRAIN.CTRL,
#                      tuneGrid=expand.grid(alpha=0, # Ridge regression
#                                           lambda=lambdas))
fit2 <- glmnet(x = data2, y = y, lambda = 1)
fit2$beta
fit2$a0
# model_ridge <- train(x=X,y=Y,
#                      method="glmnet",
#                      metric="RMSE",
#                      maximize=FALSE,
#                      trControl=CARET.TRAIN.CTRL,
#                      tuneGrid=expand.grid(alpha=0, # Ridge regression
#                                           lambda=lambdas))
fit2 <- glmnet(x = data2, y = y, lambda = 1)
# model_ridge <- train(x=X,y=Y,
#                      method="glmnet",
#                      metric="RMSE",
#                      maximize=FALSE,
#                      trControl=CARET.TRAIN.CTRL,
#                      tuneGrid=expand.grid(alpha=0, # Ridge regression
#                                           lambda=lambdas))
fit2 <- glmnet(x = data2, y = y, lambda = 1)
fit2
# model_ridge <- train(x=X,y=Y,
#                      method="glmnet",
#                      metric="RMSE",
#                      maximize=FALSE,
#                      trControl=CARET.TRAIN.CTRL,
#                      tuneGrid=expand.grid(alpha=0, # Ridge regression
#                                           lambda=lambdas))
fit2 <- glmnet(x = data2, y = y, lambda = 1, alpha = 0)
fit2
summary(fit2)
preditc(fit2, type = 'coefficients')
predict(fit2, type = 'coefficients')
# model_ridge <- train(x=X,y=Y,
#                      method="glmnet",
#                      metric="RMSE",
#                      maximize=FALSE,
#                      trControl=CARET.TRAIN.CTRL,
#                      tuneGrid=expand.grid(alpha=0, # Ridge regression
#                                           lambda=lambdas))
fit2 <- glmnet(x = data2, y = y, lambda = 1)
predict(fit2, type = 'coefficients')
# model_ridge <- train(x=X,y=Y,
#                      method="glmnet",
#                      metric="RMSE",
#                      maximize=FALSE,
#                      trControl=CARET.TRAIN.CTRL,
#                      tuneGrid=expand.grid(alpha=0, # Ridge regression
#                                           lambda=lambdas))
fit2 <- glmnet(x = data2, y = y, lambda = 1, alpha = 0)
predict(fit2, type = 'coefficients', s = bestlam)
# model_ridge <- train(x=X,y=Y,
#                      method="glmnet",
#                      metric="RMSE",
#                      maximize=FALSE,
#                      trControl=CARET.TRAIN.CTRL,
#                      tuneGrid=expand.grid(alpha=0, # Ridge regression
#                                           lambda=lambdas))
fit2 <- glmnet(x = data2, y = y, lambda = 1, alpha = 0)
fit2
predict(fit2, type = 'coefficients', s = 0)
predict(fit2, type = 'coefficients', s = 0, exact = T)
coef(fit2)
coef(fit2, exact = T)
data2
# model_ridge <- train(x=X,y=Y,
#                      method="glmnet",
#                      metric="RMSE",
#                      maximize=FALSE,
#                      trControl=CARET.TRAIN.CTRL,
#                      tuneGrid=expand.grid(alpha=0, # Ridge regression
#                                           lambda=lambdas))
fit2 <- glmnet(x = data2, y = y, lambda = 1, alpha = 0)
coef(fit2, exact = T)
predict(fit2, type = 'coefficients', s = 0, exact = T)
predict(fit2, type = 'coefficients', s = 0)
install.packages("ridge")
library("ridge", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
dataZZ = data.frame(X1 = c(0, 1, 2, 3), X2 = c(3, 2, 1, 0), Y = c(0, 1, 0, 3))
linRidgeMod <- linearRidge(Y ~ ., data = dataZZ)
linRidgeMod
linRidgeMod <- linearRidge(Y ~ ., data = dataZZ, lambda = 1)
linRidgeMod
# model_ridge <- train(x=X,y=Y,
#                      method="glmnet",
#                      metric="RMSE",
#                      maximize=FALSE,
#                      trControl=CARET.TRAIN.CTRL,
#                      tuneGrid=expand.grid(alpha=0, # Ridge regression
#                                           lambda=lambdas))
fit2 <- glmnet(x = data2, y = y, lambda = 1, alpha = 1)
coef(fit2, exact = T)
# model_ridge <- train(x=X,y=Y,
#                      method="glmnet",
#                      metric="RMSE",
#                      maximize=FALSE,
#                      trControl=CARET.TRAIN.CTRL,
#                      tuneGrid=expand.grid(alpha=0, # Ridge regression
#                                           lambda=lambdas))
fit2 <- glmnet(x = data2, y = y, lambda = 1, alpha = 0)
coef(fit2, exact = T)
linRidgeMod <- linearRidge(Y ~ ., data = dataZZ, lambda = 1)
linRidgeMod
# train the model for each parameter combination in the grid,
#   using CV to evaluate
xgb_train_1 = train(
x = as.matrix(diamonds_n[-4]),
y = as.vector(diamonds_n$price),
trControl = xgb_trcontrol_1,
tuneGrid = xgb_grid_1,
method = "xgbLinear"
)
library("caret", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
# set up the cross-validated hyper-parameter search
xgb_grid_1 = expand.grid(
nrounds = 1000,
eta = c(0.01, 0.001, 0.0001),
lambda = 0,
alpha = 0
)
# pack the training control parameters
xgb_trcontrol_1 = trainControl(
method = "cv",
number = 5,
allowParallel = TRUE
)
# train the model for each parameter combination in the grid,
#   using CV to evaluate
xgb_train_1 = train(
x = as.matrix(diamonds_n[-4]),
y = as.vector(diamonds_n$price),
trControl = xgb_trcontrol_1,
tuneGrid = xgb_grid_1,
method = "xgbLinear"
)
diamonds <- diamonds
summary(diamonds)
diamonds_n <- diamonds[-c(2, 3, 4)]
# set up the cross-validated hyper-parameter search
xgb_grid_1 = expand.grid(
nrounds = 1000,
eta = c(0.01, 0.001, 0.0001),
lambda = 0,
alpha = 0
)
# pack the training control parameters
xgb_trcontrol_1 = trainControl(
method = "cv",
number = 5,
allowParallel = TRUE
)
# train the model for each parameter combination in the grid,
#   using CV to evaluate
xgb_train_1 = train(
x = as.matrix(diamonds_n[-4]),
y = as.vector(diamonds_n$price),
trControl = xgb_trcontrol_1,
tuneGrid = xgb_grid_1,
method = "xgbLinear"
)
# set up the cross-validated hyper-parameter search
xgb_grid_1 = expand.grid(
nrounds = 10,
eta = c(0.01, 0.001, 0.0001),
lambda = 0,
alpha = 0
)
# pack the training control parameters
xgb_trcontrol_1 = trainControl(
method = "cv",
number = 5,
allowParallel = TRUE
)
# train the model for each parameter combination in the grid,
#   using CV to evaluate
xgb_train_1 = train(
x = as.matrix(diamonds_n[-4]),
y = as.vector(diamonds_n$price),
trControl = xgb_trcontrol_1,
tuneGrid = xgb_grid_1,
method = "xgbLinear"
)
xgb_train_1$results
# scatter plot of the AUC against max_depth and eta
ggplot(xgb_train_1$results, aes(x = as.factor(eta), y = max_depth, size = ROC, color = ROC)) +
geom_point() +
theme_bw() +
scale_size_continuous(guide = "none")
# set up the cross-validated hyper-parameter search
xgb_grid_1 = expand.grid(
nrounds = 10,
eta = c(0.01, 0.001, 0.0001),
lambda = c(0, 0.1, 0.01, 0.5, 0.7),
alpha = c(0, 0.1, 0.01, 0.5, 0.7)
)
# pack the training control parameters
xgb_trcontrol_1 = trainControl(
method = "cv",
number = 5,
allowParallel = TRUE
)
# train the model for each parameter combination in the grid,
#   using CV to evaluate
xgb_train_1 = train(
x = as.matrix(diamonds_n[-4]),
y = as.vector(diamonds_n$price),
trControl = xgb_trcontrol_1,
tuneGrid = xgb_grid_1,
method = "xgbLinear"
)
xgb_train_1
xgb_train_1$results
# train the model for each parameter combination in the grid,
#   using CV to evaluate
xgb_train_1 = train(metric = "ROC",
x = as.matrix(diamonds_n[-4]),
y = as.vector(diamonds_n$price),
trControl = xgb_trcontrol_1,
tuneGrid = xgb_grid_1,
method = "xgbLinear"
)
# train the model for each parameter combination in the grid,
#   using CV to evaluate
xgb_train_1 = train(metric = "AUC",
x = as.matrix(diamonds_n[-4]),
y = as.vector(diamonds_n$price),
trControl = xgb_trcontrol_1,
tuneGrid = xgb_grid_1,
method = "xgbLinear"
)
xgb_train_1
xgb_train_1$modelType
xgb_train_1$pred
xgb_train_1$metric
xgb_train_1$levels
xgb_train_1$preProcess
xgb_train_1$call
# train the model for each parameter combination in the grid,
#   using CV to evaluate
xgb_train_1 = train(metric = "AUC",
x = as.matrix(diamonds_n[-4]),
y = as.vector(diamonds_n$price),
trControl = xgb_trcontrol_1,
# tuneGrid = xgb_grid_1,
method = "xgbLinear"
)
# pack the training control parameters
xgb_trcontrol_1 = trainControl(
method = "cv",
number = 5,
allowParallel = TRUE,
summaryFunction = twoClassSummary
)
# train the model for each parameter combination in the grid,
#   using CV to evaluate
xgb_train_1 = train(metric = "AUC",
x = as.matrix(diamonds_n[-4]),
y = as.vector(diamonds_n$price),
trControl = xgb_trcontrol_1,
# tuneGrid = xgb_grid_1,
method = "xgbLinear"
)
# train the model for each parameter combination in the grid,
#   using CV to evaluate
xgb_train_1 = train(metric = "AUC",
x = as.matrix(diamonds_n[-4]),
y = as.vector(diamonds_n$price),
trControl = xgb_trcontrol_1,
tuneGrid = xgb_grid_1,
method = "xgbLinear"
)
xgb_train_1
# pack the training control parameters
xgb_trcontrol_1 = trainControl(
method = "cv",
number = 5,
allowParallel = TRUE
)
# train the model for each parameter combination in the grid,
#   using CV to evaluate
xgb_train_1 = train(metric = "AUC",
x = as.matrix(diamonds_n[-4]),
y = as.vector(diamonds_n$price),
trControl = xgb_trcontrol_1,
tuneGrid = xgb_grid_1,
method = "xgbLinear"
)
xgb_train_1$yLimits
cor.test(x = diamonds_n)
cor(x = diamonds_n)
# set up the cross-validated hyper-parameter search
xgb_grid_1 = expand.grid(
nrounds = 10,
eta = c(0.01, 0.001, 0.0001, 0.2, 0.3, 0.4),
lambda = c(0, 0.1, 0.01, 0.5, 0.7, 0.8, 1),
alpha = c(0, 0.1, 0.01, 0.5, 0.7, 0.8, 1)
)
# pack the training control parameters
xgb_trcontrol_1 = trainControl(
method = "cv",
number = 5,
allowParallel = TRUE
)
# train the model for each parameter combination in the grid,
#   using CV to evaluate
xgb_train_1 = train(
x = as.matrix(diamonds_n[-4]),
y = as.vector(diamonds_n$price),
trControl = xgb_trcontrol_1,
tuneGrid = xgb_grid_1,
method = "xgbLinear"
)
xgb_train_1$finalModel
xgb_train_1$tuneValue
xgb_train_1$bestTune
hours(00:10)
hours(1)
library("lubridate", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
hours(1)
test_time<- ymd("2011-01-01 23:00:00")+ hours(1)
ymd("2011-01-01 23:00:00")+ hours(1)
ymd_hms("2011-01-01 23:00:00")+ hours(1)
ymd_hms("2011-01-01 23:00:00")+ hours(2)
hours = 1:24
sapply(hours, function (x) ymd_hms("2011-01-01 23:00:00") + x)
sapply(hours, function (x) ymd_hms("2011-01-01 23:00:00") + hours(x))
sapply(1:24, function (x) ymd_hms("2011-01-01 23:00:00") + hours(x))
x = 1:24
for (i in x){
print(ymd_hms("2011-01-01 23:00:00") + hours(i))
}
x = 1:46
for (i in x){
print(ymd_hms("2011-01-01 23:00:00") + hours(i))
}
sapply(1:24, function (x) ymd_hms("2011-01-01 23:00:00") + hours(x))
as.POSIXct(sapply(1:24, function (x) ymd_hms("2011-01-01 23:00:00") + hours(x)))
as.POSIXct(sapply(1:24, function (x) ymd_hms("2011-01-01 23:00:00") + hours(x)), origin = "1970-01-01")
as.POSIXct(sapply(1:50, function (x) ymd_hms("2011-01-01 23:00:00") + hours(x)), origin = "1970-01-01")
as.POSIXct(sapply(1:50, function (x) ymd_hms("2011-01-01 22:00:00") + hours(x)), origin = "1970-01-01")
as.POSIXct(sapply(1:24, function (x) ymd_hms("2011-01-01 22:00:00") + hours(x)), origin = "1970-01-01")
as.POSIXct(sapply(0:23, function (x) ymd_hms("2011-01-01 22:00:00") + hours(x)), origin = "1970-01-01")
hours(0)
# x = 1:46
as.POSIXct(sapply(0:23, function (x) ymd_hms("2011-01-01 22:00:00") + hours(x)), origin = "1970-01-01")
# x = 1:46
as.POSIXct(sapply(0:23, function (x) ymd_hms("2011-01-01 23:00:00") + hours(x)), origin = "1970-01-01")
# x = 1:46
as.POSIXct(sapply(1:23, function (x) ymd_hms("2011-01-01 23:00:00") + hours(x)), origin = "1970-01-01")
# x = 1:46
as.POSIXct(sapply(0:23, function (x) ymd_hms("2011-01-01 23:00:00") + hours(x)), origin = "1970-01-01 00:00:00")
# x = 1:46
as.POSIXct(sapply(0:23, function (x) ymd_hms("2011-01-01 22:00:00") + hours(x)), origin = "1970-01-01")
ymd_hms("2011-01-01 22:00:00") + hours(0)
ymd_hms("2011-01-01 22:00:00") + hours(1)
# x = 1:46
as.POSIXct(sapply(0:23, function (x) ymd_hms("2011-01-01 22:00:00") + hours(x)), origin = "1970-01-01", tz = "UTC")
# x = 1:46
as.POSIXct(sapply(1:24, function (x) ymd_hms("2011-01-01 22:00:00") + hours(x)), origin = "1970-01-01", tz = "UTC")
# x = 1:46
as.POSIXct(sapply(1:24, function (x) ymd_hms("2011-01-01 23:00:00") + hours(x)), origin = "1970-01-01", tz = "UTC")
# x = 1:46
as.POSIXct(sapply(1:24, function (x) ymd_hms("2011-01-01 23:00:00") + hours(x)), origin = "1970-01-01", tz = "UTC")
# x = 1:46
as.POSIXct(sapply(1:24, function (x) ymd_hms("2011-01-01 20:00:00") + hours(x)), origin = "1970-01-01", tz = "UTC")
# test_time<- ymd_hms("2011-01-01 23:00:00")+ hours(2)
ymd_hms("2011-01-01 23:00:00" )+ hours(1)
# test_time<- ymd_hms("2011-01-01 23:00:00")+ hours(2)
ymd("2011-01-01") +  days(1:10)
# test_time<- ymd_hms("2011-01-01 23:00:00")+ hours(2)
ymd("2011-01-01 23:00:00") +  days(1:10)
# test_time<- ymd_hms("2011-01-01 23:00:00")+ hours(2)
ymd_hms("2011-01-01 23:00:00") + hours(1:10)
# test_time<- ymd_hms("2011-01-01 23:00:00")+ hours(2)
ymd_hms("2011-01-01 23:00:00") + hours(0:10)
# test_time<- ymd_hms("2011-01-01 23:00:00")+ hours(2)
ymd_hms("2011-01-01 23:00:00") + hours(00:10)
# test_time<- ymd_hms("2011-01-01 23:00:00")+ hours(2)
ymd_hms("2011-01-01 23:00:00") + hours(0:10)
sapply(1:24, function (x) ymd_hms("2011-01-01 20:00:00") + hours(x))
sapply(1:24, function (x) ymd_hms("2011-01-01 20:00:00") + hours(x))
# x = 1:46
as.POSIXct(sapply(1:24, function (x) ymd_hms("2011-01-01 20:00:00") + hours(x)), origin = "1970-01-01", tz = "UTC")
install.packages("manipulate")
library("manipulate", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
manipulate(plot(1:x), x = slider(1, 100))
slider(1, 100)
myHist <- function(mu){
hist(galton$child,col="blue",breaks=100)
lines(c(mu, mu), c(0, 150),col="red",lwd=5)
mse <- mean((galton$child - mu)^2)
text(63, 150, paste("mu = ", mu))
text(63, 140, paste("MSE = ", round(mse, 2)))
}
manipulate(myHist(mu), mu = slider(62, 74, step = 0.5))
data(Galton)
install.packages("Galton")
data(Galton)
myHist <- function(mu){
hist(galton$child,col="blue",breaks=100)
lines(c(mu, mu), c(0, 150),col="red",lwd=5)
mse <- mean((galton$child - mu)^2)
text(63, 150, paste("mu = ", mu))
text(63, 140, paste("MSE = ", round(mse, 2)))
}
manipulate(myHist(mu), mu = slider(62, 74, step = 0.5))
myHist <- function(mu){
# hist(galton$child,col="blue",breaks=100)
lines(c(mu, mu), c(0, 150),col="red",lwd=5)
mse <- mean((galton$child - mu)^2)
text(63, 150, paste("mu = ", mu))
text(63, 140, paste("MSE = ", round(mse, 2)))
}
manipulate(myHist(mu), mu = slider(62, 74, step = 0.5))
load(galton)
install.packages(c("devtools","UsingR"))
library("devtools", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
library("UsingR", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
data(galton)
galton = galton
View(galton)
View(galton)
myHist <- function(mu){
hist(galton$child,col="blue",breaks=100)
lines(c(mu, mu), c(0, 150),col="red",lwd=5)
mse <- mean((galton$child - mu)^2)
text(63, 150, paste("mu = ", mu))
text(63, 140, paste("MSE = ", round(mse, 2)))
}
manipulate(myHist(mu), mu = slider(62, 74, step = 0.5))
library("shiny", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
shinyUI(fluidPage(
titlePanel("Data Science FTW!"),
sidebarLayout(
sidebarPanel(
h3("Sidebar Text")
),
mainPanel(
h3("Main Panel Text")
)
)
))
source('~/R_tasks/draft.R')
runApp('myApp')
source('~/R_tasks/draft.R')
runApp('myApp')
setwd("~/myApp")
runApp()
runApp()
?builder
source('~/R_tasks/draft.R')
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp('~/Shiny/Plot_Random_Numbers')
runApp('~/Shiny/Plot_Random_Numbers')
runApp('~/Shiny/reactivityApp')
runApp('~/Shiny/reactivityApp')
runApp('~/Shiny/reactivityApp')
runApp('~/Shiny/reactivityApp')
runApp('~/Shiny/reactivityApp')
runApp('~/Shiny/reactivityApp')
runApp('~/Shiny/Tables')
runApp('~/Shiny/Tables')
runApp('~/Shiny/Tables')
summary(mtcars)
source('~/R_tasks/draft.R')
runApp('~/Shiny/Tables')
